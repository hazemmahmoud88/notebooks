{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking the Smoke Caused by the fires\n",
    "\n",
    "In this example we show how to use [HRRR Smoke Experimental dataset](https://data.planetos.com/datasets/noaa_hrrr_wrf_smoke) to analyse smoke in the US and we will also download historical fire data from [Cal Fire](https://www.fire.ca.gov/incidents) web page to visualize burned area since 2013.\n",
    "\n",
    "[The High-Resolution Rapid Refresh Smoke (HRRRSmoke)](https://data.planetos.com/datasets/noaa_hrrr_wrf_smoke) is a three-dimensional model that allows simulation of mesoscale flows and smoke dispersion over complex terrain, in the boundary layer and aloft at high spatial resolution over the CONUS domain. The smoke model comprises a suite of fire and environmental products for forecasters during the fire weather season. Products derived from the HRRRSmoke model include the Fire Radiative Power (FRP), Near-Surface Smoke (PM2.5), and Vertically Integrated Smoke, to complement the 10-meter winds, 1-hour precipitation, 2-meter temperature and surface visibility experimental forecast products. Keep in mind, that this dataset is EXPERIMENTAL. Therefore, they should not be used to make decisions regarding safety of life or property.\n",
    "\n",
    "HRRR Smoke has many different weather parameters, two of them are directly smoke related - Column-integrated mass density and Mass density (concentration) @ Specified height level above ground. First is vertically-integrated smoke and second smoke on the lowest model level (8 m). \n",
    "\n",
    "As we would like to have data about the Continental United States we will download data by using Package API. Then we will create a widget where you can choose timestamp by using a slider. After that, we will also save the same data as a GIF to make sharing the results with friends and colleagues more fun. And finally, we will compare smoke data with CAMS particulate matter data to find out if there's a colleration between them as HRRR Smoke is still experimental."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import dh_py_access.lib.datahub as datahub\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from mpl_toolkits.basemap import Basemap,shiftgrid\n",
    "import dh_py_access.package_api as package_api\n",
    "import matplotlib.colors as colors\n",
    "import warnings\n",
    "import datetime\n",
    "import shutil\n",
    "import imageio\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib as mpl\n",
    "import wget\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Please put your datahub API key into a file called APIKEY and place it to the notebook folder or assign your API key directly to the variable API_key!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = 'api.planetos.com'\n",
    "API_key = open('APIKEY').readlines()[0].strip() #'<YOUR API KEY HERE>'\n",
    "version = 'v1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, we need to define the dataset name and a variable we want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh = datahub.datahub(server,version,API_key)\n",
    "dataset = 'noaa_hrrr_wrf_smoke'\n",
    "variable_name1 = 'Mass_density_concentration_height_above_ground'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define spatial range. We decided to analyze US, where unfortunately catastrofic wildfires are taking place at the moment and influeces air quality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reftime = datetime.datetime.strftime(datetime.datetime.today(), '%Y-%m-%d') + 'T00:00:00'\n",
    "area_name = 'usa'\n",
    "today_hr = datetime.datetime.strftime(datetime.datetime.today(),'%Y%m%dT%H')\n",
    "\n",
    "latitude_north = 49; longitude_west = -127\n",
    "latitude_south = 26; longitude_east = -70.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the data with package API\n",
    "\n",
    "1. Create package objects\n",
    "2. Send commands for the package creation\n",
    "3. Download the package files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "package_hrrr = package_api.package_api(dh,dataset,variable_name1,longitude_west,longitude_east,latitude_south,latitude_north,area_name=area_name+today_hr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "package_hrrr.make_package()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "package_hrrr.download_package()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work with the downloaded files\n",
    "\n",
    "We start with opening the files with xarray. After that, we will create a map plot with a time slider, then make a GIF using the images, then we will do the same thing for closer area - California; and finally, we will download csv file about fires in California to visualize yearly incidents data as a bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd1 = xr.open_dataset(package_hrrr.local_file_name)\n",
    "dd1['longitude'] = ((dd1.lon+180) % 360) - 180\n",
    "dd1[variable_name1].data[dd1[variable_name1].data < 0] = 0\n",
    "dd1[variable_name1].data[dd1[variable_name1].data == np.nan] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are making a Basemap of the US that we will use for showing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Basemap(projection='merc', lat_0 = 55, lon_0 = -4,\n",
    "         resolution = 'h', area_thresh = 0.05,\n",
    "         llcrnrlon=longitude_west, llcrnrlat=latitude_south,\n",
    "         urcrnrlon=longitude_east, urcrnrlat=latitude_north)\n",
    "lons,lats = np.meshgrid(dd1.longitude.data,dd1.lat.data)\n",
    "lonmap,latmap = m(lons,lats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now it is time to plot all the data. A great way to do it is to make an interactive widget, where you can choose time stamp by using a slider. \n",
    "\n",
    "As the minimum and maximum values are very different, we are using logarithmic colorbar to visualize it better.\n",
    "\n",
    "On the map we can see that the areas near fires have more smoke, but it travels pretty far. Depending on when the notebook is run, we can see very different results.\n",
    "\n",
    "But first we define minimum, maximum and also colormap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmax = np.nanmax(dd1[variable_name1].data)\n",
    "vmin = 2\n",
    "\n",
    "cmap = mpl.cm.twilight.colors[:-100]\n",
    "tmap = mpl.colors.LinearSegmentedColormap.from_list('twilight_edited', cmap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadimg(k):\n",
    "    fig=plt.figure(figsize=(10,7))\n",
    "    ax = fig.add_subplot(111)\n",
    "    pcm = m.pcolormesh(lonmap,latmap,dd1[variable_name1].data[k][0],\n",
    "                norm = colors.LogNorm(vmin=vmin, vmax=vmax),cmap = tmap)\n",
    "    ilat,ilon = np.unravel_index(np.nanargmax(dd1[variable_name1].data[k][0]),dd1[variable_name1].data[k][0].shape)\n",
    "    cbar = plt.colorbar(pcm,fraction=0.024, pad=0.040,ticks=[10**0, 10**1, 10**2,10**3])\n",
    "    cbar.ax.set_yticklabels([0,10,100,1000]) \n",
    "    ttl = plt.title('Near Surface Smoke ' + str(dd1[variable_name1].time[k].data)[:-10],fontsize=20,fontweight = 'bold')\n",
    "    ttl.set_position([.5, 1.05])\n",
    "    cbar.set_label(dd1[variable_name1].units)\n",
    "\n",
    "    m.drawcountries()\n",
    "    m.drawstates()\n",
    "    m.drawcoastlines()\n",
    "    print(\"Maximum: \",\"%.2f\" % np.nanmax(dd1[variable_name1].data[k][0]))\n",
    "\n",
    "    plt.show()\n",
    "widgets.interact(loadimg, k=widgets.IntSlider(min=0,max=len(dd1[variable_name1].data)-1,step=1,value=0, layout=widgets.Layout(width='100%')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's include an image from the last time-step as well, because GitHub Preview doesn't show the time slider images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadimg(9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the function below we will save images you saw above to the local filesystem as a GIF, so it is easily shareable with others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ani(m,lonmap,latmap,aniname,smaller_area=False):\n",
    "    if smaller_area==True:\n",
    "        fraction = 0.035\n",
    "        fontsize = 13\n",
    "    else:\n",
    "        fraction = 0.024\n",
    "        fontsize = 20\n",
    "        \n",
    "    folder = './anim/'\n",
    "    for k in range(len(dd1[variable_name1])):\n",
    "        filename = folder + 'ani_' + str(k).rjust(3,'0') + '.png'\n",
    "        if not os.path.exists(filename):\n",
    "            fig=plt.figure(figsize=(10,7))\n",
    "            ax = fig.add_subplot(111)\n",
    "            pcm = m.pcolormesh(lonmap,latmap,dd1[variable_name1].data[k][0],\n",
    "            norm = colors.LogNorm(vmin=vmin, vmax=vmax),cmap = tmap)\n",
    "\n",
    "            m.drawcoastlines()\n",
    "            m.drawcountries()\n",
    "            m.drawstates()\n",
    "            cbar = plt.colorbar(pcm,fraction=fraction, pad=0.040,ticks=[10**0, 10**1, 10**2,10**3])\n",
    "            cbar.ax.set_yticklabels([0,10,100,1000]) \n",
    "            ttl = plt.title('Near Surface Smoke ' + str(dd1[variable_name1].time[k].data)[:-10],fontsize=fontsize,fontweight = 'bold')\n",
    "            ttl.set_position([.5, 1.05])\n",
    "            cbar.set_label(dd1[variable_name1].units)\n",
    "            ax.set_xlim()\n",
    "            if not os.path.exists(folder):\n",
    "                os.mkdir(folder)\n",
    "            plt.savefig(filename,bbox_inches = 'tight',dpi=150)\n",
    "            plt.close()\n",
    "\n",
    "    files = sorted(os.listdir(folder))\n",
    "    images = []\n",
    "    for file in files:\n",
    "        if not file.startswith('.'):\n",
    "            filename = folder + file\n",
    "            images.append(imageio.imread(filename))\n",
    "    kargs = { 'duration': 0.3,'quantizer':2,'fps':5.0}\n",
    "    imageio.mimsave(aniname, images, **kargs)\n",
    "    print ('GIF is saved as {0} under current working directory'.format(aniname))\n",
    "    shutil.rmtree(folder)\n",
    "make_ani(m,lonmap,latmap,'hrrr_smoke.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are interested in California fires right now, it would make sense to make animation of only California area as well. So people can be prepared when smoke hits their area. The model has pretty good spatial resolution as well - 3 km, which makes tracking the smoke easier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latitude_north_cal = 43; longitude_west_cal = -126.\n",
    "latitude_south_cal = 30.5; longitude_east_cal = -113\n",
    "\n",
    "m2 = Basemap(projection='merc', lat_0 = 55, lon_0 = -4,\n",
    "         resolution = 'h', area_thresh = 0.05,\n",
    "         llcrnrlon=longitude_west_cal, llcrnrlat=latitude_south_cal,\n",
    "         urcrnrlon=longitude_east_cal, urcrnrlat=latitude_north_cal)\n",
    "lons2,lats2 = np.meshgrid(dd1.longitude.data,dd1.lat.data)\n",
    "lonmap_cal,latmap_cal = m2(lons2,lats2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_ani(m2,lonmap_cal,latmap_cal,'hrrr_smoke_california.gif',smaller_area=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will remove the package we downloaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(package_hrrr.local_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data about Burned Area from Cal Fire\n",
    "Now we will download csv file from Cal Fire web page and illustrate how many acres each year was burnt since 2013. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wget.download('https://www.fire.ca.gov/imapdata/mapdataall.csv',out='acres_burned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datain = pd.read_csv('acres_burned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we convert `incident_dateonly_created` column to datetime, so it's easier to group data by year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datain['incident_dateonly_created'] = pd.to_datetime(datain['incident_dateonly_created'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can see the data from `acres_burned.csv` file. It has information about each incident. This time we only compute total acres burned each year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing yearly sums. In some reason there's many years without much data, so we will filter it out. Also, reseting index, as we don't want dates to be as an index and making year column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burned_acres_yearly = datain.resample('1AS', on='incident_dateonly_created')['incident_acres_burned'].sum()\n",
    "burned_acres_yearly = burned_acres_yearly[burned_acres_yearly.index > datetime.datetime(2012,1,1)]\n",
    "burned_acres_yearly = burned_acres_yearly.reset_index()\n",
    "burned_acres_yearly['year'] = pd.DatetimeIndex(burned_acres_yearly.incident_dateonly_created).year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the computed data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burned_acres_yearly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will make a bar chart of the data. We are using [seaborn](https://seaborn.pydata.org/) this time for plotting the data and to visualize it better, we added colormap to bar chart as well. \n",
    "Image will be saved into the working directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,6))\n",
    "pal = sns.color_palette(\"YlOrRd_r\", len(burned_acres_yearly))\n",
    "rank = burned_acres_yearly['incident_acres_burned'].argsort().argsort()\n",
    "\n",
    "sns.barplot(x='year',y='incident_acres_burned',data=burned_acres_yearly,ci=95,ax=ax,palette=np.array(pal[::-1])[rank])\n",
    "ax.set_xlabel('Year',fontsize=15)\n",
    "\n",
    "ax.set_ylabel('Burned Area [acres]',fontsize=15)\n",
    "ax.grid(color='#C3C8CE',alpha=1)\n",
    "ax.set_axisbelow(True)\n",
    "ax.spines['bottom'].set_color('#C3C8CE')\n",
    "ax.spines['top'].set_color('#C3C8CE')\n",
    "ax.spines['left'].set_color('#C3C8CE')\n",
    "ax.spines['right'].set_color('#C3C8CE')\n",
    "ttl = ax.set_title('Burned Area in California',fontsize=20,fontweight = 'bold')\n",
    "ttl.set_position([.5, 1.05])\n",
    "ax.tick_params(labelsize=15,length=0)\n",
    "plt.savefig('acres_burned_cali.png',dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 1,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
